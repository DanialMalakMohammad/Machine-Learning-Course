import numpy as np
import pickle
import sklearn.cluster
import matplotlib.pyplot as plt

Accuracies  = [0.9675566096268594, 0.9679424122108881, 0.9675561618865154, 0.9676851608545237, 0.9675561618865154, 0.9678141598225319, 0.9678141598225319, 0.9678134132428798, 0.9675563107875513, 0.9615067997993748, 0.926864763544548]
non_zero_coef_ratio =  [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]
clf = 'Logistic Regression'
penalty='L2'


Accuracies = [0.9688427175076454, 0.9687141662799812, 0.9689718653766896, 0.9689717164756536, 0.9689715665373814, 0.9690999688640097, 0.9692294155723621, 0.9698735149317153, 0.967297866148427, 0.933949401496435, 0.8057092207971451]
non_zero_coef_ratio = [1.0, 1.0, 1.0, 1.0, 1.0, 0.9998514557338086, 0.9985145573380867, 0.8207070707070707, 0.24420677361853832, 0.03832442067736185, 0.006833036244800951]
clf = 'Logistic Regression'
penalty='L1'

Accuracies =   [0.9680743964602687, 0.9665295451010505, 0.9692298643499423, 0.9684563193194732, 0.9424490015450258, 0.9221012146478303, 0.886821195385722, 0.8672560593013849, 0.8246414665892744, 0.7905296008475085, 0.7700540041161195]
non_zero_coef_ratio =   [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9998514557338086, 0.9998514557338086, 0.9994058229352347, 0.9998514557338086]
clf = 'Linear SVM'
penalty='L2'


Accuracies =  [0.9639535974784542, 0.9654987487142169, 0.9696168622912035, 0.9630486658772207, 0.8874723107267801, 0.8637795054682508, 0.7930946486146304, 0.7546042920549958, 0.7059464453099258, 0.48152201926775406, 0.15450012974904354]
non_zero_coef_ratio = [0.6081402257872847, 0.3530897207367796, 0.2043969102792632, 0.10308972073677956, 0.030897207367795602, 0.020202020202020204, 0.012477718360071301, 0.006981580510992276, 0.004010695187165776, 0.0016339869281045752, 0.00029708853238265005]
clf = 'Linear SVM'
penalty='L1'

accuracies=[0.9536679536679536, 0.924066924066924, 0.915057915057915, 0.31016731016731014, 0.17117117117117117, 0.17117117117117117, 0.1879021879021879, 0.15315315315315314, 0.15186615186615188, 0.14157014157014158, 0.1287001287001287]
non_zero_coef_ratio=[0.5138218943998616, 0.43935887175272004, 0.26415175964179877, 0.0619930349765309, 0.023166273712444032, 0.027632973545889124, 0.018299409487140664, 0.024377582141852868, 0.04784668296164911, 0.0485604897146936, 0.0728299193182064]
non_zero_coef_mean=[-0.0012592921300287013, -0.00020892376919545773, 0.0009705267563054048, 0.0011942332785754722, 0.0013276035922475116, 0.0009539807934903034, 0.002924665850188823, 0.0009494600083348451, 0.0015698027326537035, 0.001462276223901553, 0.0003998379810197039]
clf = 'Neural Network'
penalty='L2'

accuracies=[0.9343629343629344, 0.5817245817245817, 0.21364221364221364, 0.1274131274131274, 0.15057915057915058, 0.14414414414414414, 0.15701415701415702, 0.15444015444015444, 0.17631917631917632, 0.15057915057915058, 0.15958815958815958]
non_zero_coef_ratio=[0.9980424390560446, 0.9977828729640285, 0.9976639051718544, 0.9977720577101945, 0.9974584153490083, 0.997079881464818, 0.9975557526335143, 0.9977396119486924, 0.9977287966948585, 0.9976530899180204, 0.9979126560100365]
non_zero_coef_mean=[0.00026271417277812844, 0.001765048697260668, 0.0015733848615280363, 0.0017259649139436549, 0.0016063653026554229, 0.001431069294718449, 0.0015273715516848404, 0.001826709350747827, 0.0016575432997496136, 0.001499898687659678, 0.001964997056000199]
clf = 'Neural Network'
penalty='L1'


Accuracies          =     [0.9675566096268594, 0.9679424122108881, 0.9675561618865154, 0.9676851608545237, 0.9675561618865154, 0.9678141598225319, 0.9678141598225319, 0.9678134132428798, 0.9675563107875513, 0.9615067997993748, 0.926864763544548]
non_zero_coef_ratio =     [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]
non_zero_coef_mean  =     [0.028698143777518795, 0.028703294137539782, 0.029120929561890108, 0.02845386026340359, 0.029628570885337283, 0.028529983139965104, 0.028461755416071582, 0.028033630788580906, 0.023406771646678894, 0.013650313611866902, 0.007965741430481468]
clf='Logistic Regression'
penalty='L2'

Accuracies =   [0.9680743964602687, 0.9665295451010505, 0.9692298643499423, 0.9684563193194732, 0.9424490015450258, 0.9221012146478303, 0.886821195385722, 0.8672560593013849, 0.8246414665892744, 0.7905296008475085, 0.7700540041161195]
non_zero_coef_ratio =   [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9998514557338086, 0.9998514557338086, 0.9994058229352347, 0.9998514557338086]
non_zero_coef_mean  =     [0.016628287878465042, 0.016749782795681924, 0.01674911054322412, 0.01676298138018625, 0.016424436851222395, 0.016763745108559512, 0.016762480215477736, 0.01682794242597516, 0.016370539715996776, 0.013041892663367386, 0.011093211560926004]
clf='Linear SVM'
penalty='L2'

penalties = [0.0001, 0.0002, 0.0005, 0.001, 0.002, 0.005, 0.01, 0.1, 1, 10, 100]
penalties=np.array(penalties)
Accuracies=np.array(Accuracies)
non_zero_coef_ratio=np.array(non_zero_coef_ratio)

ss=np.arange(len(penalties))

plt.cla

plt.title(clf+' '+penalty+', Accuracy')
plt.xlabel('Penalty ')
plt.ylabel('non_zero_coef_mean ')
plt.plot(ss, accuracies, 'go-')
plt.xticks(ss, penalties)
plt.show()


exit()




#aa=np.loadtxt('Feature_Selection/Variance_Threshold/5.txt',)
#print(aa.astype(int))




#Accuracies= [0.7700540041161195, 0.7905296008475085, 0.8246414665892744, 0.8672560593013849, 0.886821195385722,0.9221012146478303, 0.9424490015450258, 0.9684563193194732, 0.9692298643499423, 0.9665295451010505,0.9680743964602687]
#non_zero_coef_ratio= [0.9998514557338086, 0.9994058229352347, 0.9998514557338086, 0.9998514557338086, 1.0, 1.0, 1.0,1.0, 1.0, 1.0, 1.0]
#non_zero_coef_count =   [341   , 198   , 114   , 57   , 17   , 11   , 7   ,  3 , 2,  0,   0]
#penalties =             [0.0001, 0.0002, 0.0005, 0.001, 0.002, 0.005, 0.01, 0.1, 1, 10, 100]
#Accuracies =            [0.9639535974784542, 0.9654987487142169, 0.9696168622912035, 0.9630486658772207, 0.8874723107267801, 0.8637795054682508, 0.7930946486146304, 0.7546042920549958, 0.7059464453099258, 0.48152201926775406, 0.15450012974904354]
#non_zero_coef_ratio =   [0.6081402257872847, 0.3530897207367796, 0.2043969102792632, 0.10308972073677956, 0.030897207367795602, 0.020202020202020204, 0.012477718360071301, 0.006981580510992276, 0.004010695187165776, 0.0016339869281045752, 0.00029708853238265005]

#feature_counts = [5,10,50,100]


#non_zero_coef_ratio =[1.0, 1.0, 1.0, 1.0, 1.0, 0.9998514557338086, 0.9985145573380867, 0.8207070707070707, 0.24420677361853832, 0.03832442067736185, 0.006833036244800951]

#penalties =         [0.0001,    0.0002, 0.0005, 0.001,  0.002,  0.005,  0.01,   0.1, 1,     10, 100]
#non_zero_coef_count=[561,       561,    561,    561,    561,    560,    560,    460, 137,   21, 3]


#[100    , 50    ,10     ,5]
#[1      ,1  ,10,10]




# 10
# nl=[]
#
# for tt in non_zero_coef_ratio:
#     nl.append(int(tt*561))
#
# print(nl)
# exit()
#
#
# print(561*non_zero_coef_ratio)
# exit()
#
#
# Accuracies.reverse()
# non_zero_coef_ratio.reverse()
# print('# Accuracies =  ',Accuracies)
# print('# non_zero_coef_ratio =  ',non_zero_coef_ratio)
